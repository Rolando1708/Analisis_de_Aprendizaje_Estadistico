{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d323cad6-0afa-4c28-9ce0-07c970efdb6b",
   "metadata": {},
   "source": [
    "# Examen – Modelos de Ensamble: Árboles y Boosting\n",
    "\n",
    "**Instrucciones**: Justifica cada respuesta de manera clara. Usa fórmulas donde aplique y sé específico en tus argumentos.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección I: Árboles de decisión (15 puntos)\n",
    "\n",
    "### 1. (5 pts)  \n",
    "Explica cómo se construye un árbol y qué criterio usa para decidir los splits. Explica tanto para el caso de clasificación como de regresión.\n",
    "\n",
    "Un arbol de decisión consistente en diferentes niveles de clasificación el criterio que se utiliza para los splits en el caso de la clasificación se busca maximizar la pureza de las clases en cada nodo, lo que el algoritmo hace es que de acuerdo a todas las caracteristicas y los umbrales posibles de división, el algoritmo elige el umbral que maximize la pureza o que reduzca lo más posible la impureza donde queda la siguiente fórmula \n",
    "\n",
    "$$\n",
    "\\text{Ganancia de Impureza} = \\text{Impureza Nodo Padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "En el caso de la regresión es similar, el algoritmo de acuerdo a las caracteristicas y los umbrales posibles de división, selecciona el umbral el cual tenga la mayor reducción de varianza, queda la siguiente formula en el caso de regresión:\n",
    "\n",
    "$$\n",
    "\\text{Reducción de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)\n",
    "$$\n",
    "\n",
    "### 2. (5 pts)  \n",
    "Da un ejemplo de sobreajuste en un árbol de decisión. Explica cómo se podría evitar sin necesidad de usar ensambles.\n",
    "\n",
    "Un sobreajuste puede existir cuando el arbol es demasiado profundo, ya que esto puede significar que el algoritmo esta memorizando los datos, para poder evitar esto se puede utiliza max_depth para poner un limite de la profundidad del arbol, y también min_samples_leaf para poder hacer que el arbol calcule una predicción que sea signficativa.\n",
    "\n",
    "### 3. (5 pts)  \n",
    "Si te fijas, en clase nunca hicimos escalamiento (`StandardScaler`). ¿Por qué los árboles no lo necesitan?\n",
    "\n",
    "En cada nodo solamente hay una variable del dataset, donde se hace una división la cual el modelo vea conveniente, entonces no es necesario escalar los datos ya que no se toman todas las variables al mismo tiempo\n",
    "\n",
    "---\n",
    "\n",
    "## Sección II: Random Forest (20 puntos)\n",
    "\n",
    "### 4. (10 pts)  \n",
    "Explica cómo funciona un Random Forest. ¿En qué se basa? ¿Por qué es una buena idea?\n",
    "\n",
    "En el random forest se utiliza bootstrap, con los datos generados con este mismo se entrena el modelo y con esto se hace una predicción, al realizar las predicciones se calcula la media de estas, esta siendo la predicción del random forest.\n",
    "A la hora de clasificar en lugar de utilizar la media de las predicciones, se utiliza la que más se repite.\n",
    "\n",
    "### 5. (10 pts)  \n",
    "Menciona dos ventajas y dos desventajas del Random Forest comparado con un solo árbol. Sé específico, no generalices.\n",
    "\n",
    "Ventajas: Al utilizar el random forest estas reduciendo la posibilidad de que exista overfitting, ya que estas promediando multiples arboles los cuales contienen diferentes conjuntos de datos de tu mismo dataset, a diferencia de solamente tener un arbol.\n",
    "Utilizando boostrap en el random forest haces que tu modelo utilice menos los outliers ya que se resamplean tus datos que tienes, a diferencia de solamente un arbol donde si en el dataset existen datos atipicos los va a considerar.\n",
    "\n",
    "Desventajas: Al utilizar un solo arbol es más facil el ver como es que llego a cierta predicción, pero utilizando Random Forest al utilizar tantos arboles es muy dificil el seguir el paso de como fue que se llego a la predicción final, dandole menos interpretabilidad. Al utilizar tantos arboles se considera un costo computacional más grande a diferencia de solamente tener un arbol.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección III: Gradient Boosting (25 puntos)\n",
    "\n",
    "### 6. (10 pts)  \n",
    "Explica, paso a paso, cómo funciona el algoritmo de **Gradient Boosting**. Incluye el concepto de residuales y cómo se minimiza la pérdida en cada iteración.\n",
    "\n",
    "El Gradient Boosting basicamente lo que hace es combinar modelos debiles para poder crear uno mejor, en este caso el modelo crea una predicción inicial y calcula el error de esta predicción para poder crear un modelo que pueda predecir este error, esto se hace de manera iterativa considerando un factor de aprendizaje hasta que llegue a converger.\n",
    "\n",
    "### 7. (15 pts)  \n",
    "¿Cuál es la diferencia entre Gradient Boosting y Random Forest en términos de cómo combinan los árboles? \n",
    "\n",
    "El Gradient Boosting como ya se menciono anteriormente inicia con una predicción inicial y en base a esta predicción se calcula su error para posteriormente crear un modelo que puede predecir el error y entonces ahora con esta nueva predicción se calcula su error y así se va haciendo de manera iterativa, mientras que en un Random Forest crea muchos arboles y promedia estos mismos, se podria decir que mientras Random Forest utiliza muchos arboles, el Gradient Boosting utiliza uno base y de ese va actualizando sus predicciones hasta disminuir el error del modelo lo más posible.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Sección IV: XGBoost (20 puntos)\n",
    "\n",
    "### 8. (10 pts)  \n",
    "Explica cómo XGBoost optimiza el proceso de boosting usando una expansión de Taylor de segundo orden.\n",
    "\n",
    "Se optimiza ya que la pérdida original tiene la posibilidad de poder ser arbitraria y no tiene una forma facil de minimizar cuando se agrega a un nuevo arbol.\n",
    "Al utilizar Taylor se aproxima a una función cuadratica la cual si permite minimizar facilmente cuando se construye un arbol.\n",
    "\n",
    "### 9. (5 pts)  \n",
    "¿Qué es el *similarity score*? ¿Qué es el *output value*? \n",
    "¿De dónde salen estas fórmulas y cuál es su interpretación?\n",
    "\n",
    "La formula del output value es la siguiente:\n",
    "\n",
    "$$\n",
    "\\textbf{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "El output value calcula el valor oprtimo que predice una hoja\n",
    "\n",
    "La formula del similarity score es la siguiente\n",
    "\n",
    "$$\n",
    "\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "La cual calcula la calidad que tiene una hoja\n",
    "\n",
    "### 10. (5 pts)  \n",
    "XGBoost y otros modelos de gradient boosting permiten evaluar la importancia de las variables con diferentes métricas: `weight` y `gain`. Explica qué representa cada una. ¿Cuál crees que es más útil para interpretar un modelo y por qué?\n",
    "\n",
    "Weight indica el número de veces que se utilizo una variable en las hojas del modelo, entre más se utilice una variable esto basicamente indica que es una variable que el modelo considera importante.\n",
    "\n",
    "Gain indica la mejora del modelo cada vez que se utiliza cierta variable, es decir son las variables que mejor funcionan a la hora de disminuir el error del modelo cada vez que se utilizan.\n",
    "\n",
    "Ambas son utiles para interpretar un modelo, aunque gain se podria considerar mejor ya que te muestra las mejores variables a la hora de reducir el error del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección V: LightGBM y CatBoost (15 puntos)\n",
    "\n",
    "### 10. (5 pts)  \n",
    "Explica qué es **histogram-based splitting** y cómo lo implementa LightGBM para ganar velocidad.\n",
    "\n",
    "Es una tecnica para poder acelerar la costruccion de arboles, agrupa los datos en bins y LightGBM construye histogramas con esos bins, para posteriormente calcular los cortes optimos por bins, no con todos los valores individuales lo cual lo hace un proceso más rapido ya que no tomaría en cuenta tantos recortes posibles.\n",
    "\n",
    "\n",
    "### 11. (5 pts)  \n",
    "¿Qué problema específico resuelve CatBoost respecto al manejo de variables categóricas? ¿Cómo lo hace?\n",
    "\n",
    "Catboost pude manejar variables categoricas sin necesidad de hacer encoding manual, primero ordena de manera aleatoria las filas y a cada una codifica la categoria con el promedio acumulado del target de las observaciones anteriores de la misma categoria y nunca se incluye el target de la fila actual al momento de hacer el calculo para no provocar data leakage, en este encoding que utiliza Catboost se generan varias permutaciones que son aleatorias en el dataset, calcula el encoding acumulado en cada una y promedia los resultados para poder hacer el encoding más estable y reducir la varianza, Catboost también utiliza una suavizacion, la cual es parecida al smoothing bayesiano, para suavizar lso valores extremos y que no se generen codifiaciones estables en las categorias\n",
    "\n",
    "$$ encoding = \\frac{\\sum y  + a * \\mu}{n + a} $$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- ∑y = suma de los targets anteriores para esa categoría\n",
    "- n = número de ocurrencias anteriores\n",
    "- μ = promedio global del target\n",
    "- a = parámetro de suavizamiento\n",
    "\n",
    "### 12. (5 pts)  \n",
    "Compara LightGBM y CatBoost: ¿cuándo usarías uno sobre el otro? Sé claro y justifica en base a tipo de datos, velocidad o precisión.\n",
    "\n",
    "Catboost podría ser utilizado cuando el dataset sea pequeño y cuando tengas bastantes variables categoricas sin preprocesamiento, ambas funcionan cuando quieres interprertabilidad de SHAP pero Catboost en este caso daria mejores resultados, cuando necesites buen rendimiento sin mucho tuning.\n",
    "\n",
    "En el caso de LightGBM es recomendado cuando tienes un dataset muy grande, cuando quieras tuning automatico y de igual manera tiene un entrenamiento rápido que en general tiene buenos resultados.\n",
    "\n",
    "## Sección VI: Power Analysis (5 puntos)\n",
    "\n",
    "### 13. (5 pts)  \n",
    "¿Qué es un *power analysis* y para qué sirve? ¿En qué contexto lo hemos usado en clase y por qué es importante antes de correr un experimento?\n",
    "\n",
    "El power analysis es una tecnica la cual determina el tamaño de muestra necesario para poder realizar un cambio en alguna estadistica, esto ayuda a poder minimizar el riesgo de no poder encontrar resultados porque no existen suficientes datos.\n",
    "Esta tecnica se utiliza en A/B testing para poder ver cual es el tamaño de muestra necesario para poder aplicar esta prueba.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782e43b-cda4-43f3-998c-f3f535111a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd5493f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
