{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fb4a27-9f28-435e-8a52-8a9351c7d47d",
   "metadata": {},
   "source": [
    "# Examen 4\n",
    "\n",
    "## Sección 1: Interpretabilidad con SHAP (25 puntos)\n",
    "\n",
    "**1.1** (Teoría, 10 pts)  \n",
    "¿Qué representa un valor SHAP en el contexto de un modelo de machine learning? ¿Cuál es su fundamento teórico?\n",
    "\n",
    "Para los SHAP Values se sacan todas las permutaciones posibles del dataset y se calcula la contribución marginal de cada una de estas variables en cada permutación. Para poder obtener el SHAP Value se promedian las contribuciones marginales de cada variable.\n",
    "Basicmamente te dice como de tu modelo base o tu valor esperado se llega hasta la predicción realizada.\n",
    "\n",
    "El fundamento teórico proviene de la teoría de juegos cooperativos y son utilizados para asignar de manera justa la predicción de un modelo entre los features la formula General del valor de Shapley es la siguiente: \n",
    "\n",
    "$$\n",
    "\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! \\cdot (n - |S| - 1)!}{n!} \\cdot \\left[ f(S \\cup \\{i\\}) - f(S) \\right]\n",
    "$$\n",
    "\n",
    "- $\\phi_i$ = valor de Shapley de la variable $i$\n",
    "- $f(S \\cup \\{i\\}) - f(S)$ = contribución marginal de $i$ al conjunto $S$\n",
    "- $\\frac{|S|! \\cdot (n - |S| - 1)!}{n!}$ = peso que representa la proporción de permutaciones donde $i$ es agregada después de $S$ y antes del resto\n",
    "\n",
    "**1.2** (Cálculo, 10 pts)  \n",
    "Un modelo predice que un cliente tendrá una probabilidad de impago del 0.78. El valor esperado del modelo es 0.5. Los SHAP values para tres variables son:  \n",
    "- Edad: +0.10  \n",
    "- Ingreso mensual: -0.05  \n",
    "- Historial crediticio: +0.23  \n",
    "\n",
    "¿La suma es consistente con la predicción? Explica.\n",
    "\n",
    "Si es consistente ya que el modelo empieza en 0.5, se menciona que la edad de la persona analizada suma 0.1, despues se muestra que el ingreso de la persona disminuye 0.05 a la predicción y por ultimo el historial crediticio suma a la predicción 0.23, que al final da la suma de la probabilidad de impago del 0.78.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección 2: K-Means Clustering (20 puntos)\n",
    "\n",
    "**2.1** (Teoría, 10 pts)  \n",
    "Explica que hace KMeans, el algoritmo, como encuentra clusters, etc.\n",
    "\n",
    "K Means es un algoritmo el cual es utilizado para poder resolver problemas de agrupamiento, donde este divide los conjuntos de datos de manera que estos se parezcan lo mas posible entre si, para poder aplicarlo primero inicializa K centroides de manera aleatoria, asigna cada punto en el centro mas cercano y por ultimo recalcula los centroides como promedio de los puntos asignados a cada grupo, el paso 2 y 3 se repiten hasta llegar a la convergencia\n",
    "\n",
    "**2.2** (Criterio, 10 pts)  \n",
    "Explica cómo usarías el método del codo (*elbow method*) y qué limitaciones tiene.\n",
    "\n",
    "Normalmente entre mayor el numero de clusters es mejor, pero en algun punto el elegir tantos deja de ser optimo, por lo que este metodo te ayuda a ver en donde se encuentra el numero optimo de clusters para el modelo. Una de las limitaciones que tiene el utilizar el metodo del codo es que existen ocasiones donde esta gráfica o el codo no es tan visible o tan claro lo que puede llegar a provocar que no se elija el numero de clusters optimos para la aplicacion de KMeans.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección 3: PCA – Análisis de Componentes Principales (20 puntos)\n",
    "\n",
    "**3.1** (Teoría, 5 pts)  \n",
    "¿Qué significa que la primera componente principal maximiza la varianza?\n",
    "\n",
    "Basicamente la primera componente principal es la que apunta hacia los datos que tenga una mayor variabilidad en el modelo, ya que tu objetivo en este caso es que los componentes principales sean los que mayor explicación de variabilidad tengan, es por eso que se busca maximizar la varianza\n",
    "\n",
    "\n",
    "**3.2** (Cálculo, 10 pts)  \n",
    "Si tienes 10 variables correlacionadas y aplicas PCA, ¿cuántas componentes necesitas para explicar al menos el 90% de la varianza? Describe cómo se respondería esta pregunta\n",
    "\n",
    "Al aplicar PCA  las 10 variables correlacionadas obtendrias 10 componentes, cada una con su respectivo porcentaje de explicación de varianza, para poder llegar al 90% tendrías que sumar los porcentajes de las varianzas explicadas de las componentes principales, por ejemplo de las 10 componentes 6 son las que suman ese 90% de la varianza esas serían las necesarias.\n",
    "\n",
    "**3.3** (Aplicación, 5 pts)  \n",
    "¿Tiene sentido usar PCA y usar todas las componentes para predecir? Justifica tu respuesta.\n",
    "\n",
    "No mucho ya que si se utilizan todas las componentes a la hora de hacer la predicción seria como utilizar todas las variables, el punto de calcular los componentes es para reducir la dimensionalidad entonces usarías los mejores solamente para poder hacer la predicción.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección 4: Causalidad y Meta-Learners (35 puntos)\n",
    "\n",
    "**4.1** (Conceptual, 10 pts)  \n",
    "Define el estimando CATE y da un ejemplo de aplicación en negocios.\n",
    "\n",
    "El CATE es el efecto estimado del tratamiento para individuos con alguna caracteristica, este se podría utilizar en negocios a la hora de querer aplicar descuentos, se podria estimar como aplicar estos descuentos en diferentes productos podría afectar a las ventas del negocio\n",
    "\n",
    "**4.2** (Comparación, 15 pts)  \n",
    "Explica las diferencias entre los siguientes enfoques para estimar CATE:\n",
    "- S-Learner  \n",
    "- T-Learner  \n",
    "- X-Learner  \n",
    "\n",
    "Incluye ventajas y desventajas.\n",
    "\n",
    "El S-Learner entrena solo un modelo, en el S-Learner entrenas tu modelo con las variables originales y despues modificas los datos para que todos tengan tratamiento(1) y también para que nadie tenga tratamiento(0), en este modelo para calcular el CATE restas la diferencia de las predicciones de tratamiento menos las de sin tratamiento\n",
    "\n",
    "Ventajas:\n",
    "- Al ser solamente un modelo se podría considar que es simple\n",
    "\n",
    "Desventejas:\n",
    "- En el caso que el modelo considere al tratamiento como irrelevante o muy poco relevante para la predicción, podría llegar a provocar que el CATE no sea calculado de manera correcta\n",
    "\n",
    "El modelo T-Learner entrena dos modelos de manera independiente, uno donde el tratamiento es de 1 o donde todos tienen el tratamiento y en el otro donde el tratamiento es de 0 o que nadie tiene el tratamiento, despues de calcular cada uno utilizas ambos para poder hacer prediccion con respecto a los datos originales, donde uno esta inclinado hacia los que si tiene tratamiento y el otro esta inclinado hacia los que no. Al final para calcular el CATE se utilizan las predicciones de ambos modelos restando el del modelo con tratamiento(1) menos el modelo sin el tratamiento(0).\n",
    "\n",
    "Ventajas:\n",
    "- Al hacer un modelo para cada grupo, lo hace más flexible debido a que como se enfoca en un modelo en los que el tratamiento es 1 y el otro donde el tratamiento es 0, encuentra de mejor manera las diferencias entre los dos grupos\n",
    "\n",
    "Desventajas \n",
    "- Puede llegar a tener problemas si lso grupos llegan a tener mucha diferencia entre si, dando resultados que no serían tan buenos\n",
    "\n",
    "El X-Learner en primeras es similar al T-Learner ya que también entrena dos modelos con 1 y 0, despues que se tienen ambos modelos se calcula D0 y D1, en donde D0 es el calculo de lo que paso menos lo que pasaría sin tratamiento, en el caso de D1 lo obtienes de lo que habria pasado menos lo que paso, despues se entrenan dos modelos utilizando D0 y D1 respectivamente. Por ultimo se calcular el propensity score el cual te da la probabilidad de tener el tratamiento condicional a X.\n",
    "\n",
    "Ventajas:\n",
    "- Este modelo es el mejor de los tres cuando llega a funcionar, funciona mejor cuando existe diferencia entre los grupos\n",
    "- No siempre llega a funcionar y es bastante costoso al utilizar otros modelos como lo es el T-Learner\n",
    "\n",
    "**4.3** (Implementación, 10 pts)  \n",
    "Te entregan un dataset con una columna `treatment` (0/1), un `outcome`, y varias covariables. ¿Cómo entrenarías un X-Learner paso a paso?\n",
    "\n",
    "Primero se realizan dos modelos con tratamiento(1) y sin tratamiento(0) como se hace en el T-Learner, posteriormente se calcula D0 y D1, despues entrenarias un modelo utilizando D0 y otro utilizando D1, ya que los tienes haces la preddicon del CATE condicional dentro de cada uno de los grupos, para lograr esto entrenas md0 con los datos con tratmiento como X y D0 como y de forma inversa entrenas md1 con los datos sin tratamiento X y D1 como y. Por ultimo calculas el propensity score, donde calculas un modelo de clasificacion en donde tus valores originales son X y la y es para cuando tienen tratamiento o no, ya con el propensity score calculado ponderas el CATE para darle el peso correspondeinte dada la probabilidad de tener o no tratamiento\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
